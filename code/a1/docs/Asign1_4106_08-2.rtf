{\rtf1\ansi\ansicpg1252\cocoartf949\cocoasubrtf350
{\fonttbl\f0\froman\fcharset0 TimesNewRomanPSMT;}
{\colortbl;\red255\green255\blue255;\red255\green13\blue16;\red0\green0\blue255;}
{\info
{\author  }
{\*\company  }}\vieww12440\viewh14820\viewkind1
\deftab720
\pard\tx930\pardeftab720\li270\fi-10\ri-580\qj

\f0\b\fs28 \cf0 COMP 4106 Assignment 1: Machine Learning\
\pard\tx930\pardeftab720\li270\fi-10\ri-580\qj

\fs26 \cf0 \
\pard\tx930\pardeftab720\li270\fi-10\ri-580\qj

\b0\fs24 \cf0 \
\pard\tx930\pardeftab720\li270\fi-10\ri-580\qj

\b \cf0 1. Write a program to do 
\fs26 \cf2 decision tree induction
\fs24 \cf0  
\b0 as explained in class. Test your code against the small weather_nominal data. Have your program output the resulting decision tree. The output should be neatly indented for easy viewing.\

\b Note: your code should follow fairly closely the pseudocode given in the notes. Write the program yourself, i.e. do not try to download a program from the web (the TA will check for this)!
\b0 \
\

\b 2.
\b0  Calculate the Weather_nominal problem by hand to see if it matches the decision tree generated by your program. Present all the calculations in your documentation and draw the resulting tree. (NOTE: To reduce the amount of work here, just do hand-calculations for one branch of the tree, ie: for the root node + 1 of its children + 1 of that child\'92s children, and so on down to a leaf. Do this, however, for the 
\i\b deepest
\i0\b0  leaf in your program\'92s output, if there should happen to be variation in leaf depth).\
\

\b 3.
\b0  Run your program on the mushroom data and the voting data (separately, of course). For both data sets do the following...\
\
Divide the data into 2 halves (50-50 split), the training set and the testing set.\
(PLEASE NOTE: When dividing into a 50-50 split, take the 1\super st\nosupersub  half of the data as training, and the 2\super nd\nosupersub  half as testing, NOT alternating rows, and take the data in the order it appears in the file, NOT in a randomized order. It is important that you do this for consistency. It removes ambiguity for you, and simplifies marking for the TA. It\'92s win-win for everybody)\

\b b)
\b0  Train your program using the training set (ie: produce a tree).\
Take the produced decision tree and use it to determine the categories/classes\
of the cases in the testing set.\

\b c)
\b0  Compare the categories produced by your decision tree on the testing set with the actual categories provided in the testing set. Calculate the percentage of cases that were properly categorized by the decision tree.\

\b d) What would be the expected accuracy (percentage of cases properly categorized) if the cases were categorized randomly? Is the accuracy of the decision tree better than if it were due to chance?\
\pard\tx930\pardeftab720\li270\fi-10\ri-580\qj

\b0 \cf0 \
\pard\tx930\pardeftab720\li270\fi-10\ri-580\qj

\b \cf0 4 a)
\b0  Run your program on the poker data set. What was the accuracy obtained in determining poker hands from 5 cards and what was the size of the tree? How does this compare to the mushroom set? \

\b b)
\b0  What does the decision tree actually tell you about how to determine the poker hand ? Is the decision tree method a good way to classify poker hands?\
\

\b 5.
\b0  Note that in both the mushroom and the voting data some values are missing (indicated by \'91?\'92). One simple way to handle missing values for attributes in a row of the data file is to treat the missing value (often a question mark) as just another potential value for that attribute. Try to come up with another "better" way to handle missing attribute values and implement it. Compare the two methods, using a 50-50 split each time, for mushroom data. For each of the 2 methods, run your program several times and take the average value. How many times? Repeat each method enough times that the standard deviation on your average is less than 10% of its value.\
\

\b 6.
\b0  Randomize the mushroom data set (shuffle the rows from the data files before building the tree) and repeat part 3. Compare and contrast your results to those found in part 3. Be prepared to give good reasons for the observed differences (if any) to the TA.\
\

\b 7.
\b0  Take as the training set a 
\i RANDOM
\i0  subset of cases from the mushroom data set. Let that training set be: \
\

\b a)
\b0 \cf3  \cf0 10%, 20%, 30%,\'85 80%, 90%, and 100% of the total cases (note: of course, there\'92s no need to randomize for 100%)\
\

\b b) 
\b0 For each of the 10 cases above, time how long it takes to run your program on these different sized training cases. (Note: If your program should happen to run TOO FAST to time it, just run it many times and divide the time by the number of runs). Do not include the time it takes to randomize the data, just the tree-building time.\
\

\b c) 
\b0 What is the empirical time complexity found? Sketch a graph of the 10 data points with time on the vertical axis using a program such as Excel. Does it match the algorithmic time complexity expected? If not, try to come up with reasons that would explain the discrepancy. (Note: we are not necessarily expecting a discrepancy).\
\

\b 8.
\b0  It is claimed that randomly choosing the attribute on which to split when building the tree will produce, on average, trees that are twice as large (in terms of the number of nodes). Implement this variation on tree-building and test whether the claim is true. Try this with more than one data file. Again, repeat this enough times that the standard deviation of your average is less than 10% of its value.\
\

\b BONUS:\
\pard\tx930\pardeftab720\li270\fi-10\ri-580\qj

\b0 \cf0 For bonus marks do the following:\
Modify your program so that it can handle 
\b numeric
\b0  data (integer or floating point numbers). Test your program with the weather_numerical, iris, and labor data sets. Be prepared to explain to the TA how numerical data is handled, in contrast to nominal values\cf3 .\cf0 \
\
NOTE: Be prepared to run your code for the TA. He will specify a data set to run, whether or not it should be randomized, what the percentages should be for testing vs training, and which of your 2 missing-value-handling methods to use. Unless you absolutely cannot avoid it, do not code your program such that this request requires you to RECOMPILE the code in front of the TA. Use command-line parameters or a menu! It is your responsibility to make sure (BEFORE the due date) that you will be able to run your program on a machine in the TA room (Herzberg), or on a laptop that you will bring with you! If this is impossible, make arrangements with the TA to demo your assignment at another place and time.\
All of the testing data can be found on the course homepage.\
}